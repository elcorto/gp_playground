{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41546670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91b5fc16",
   "metadata": {},
   "source": [
    "# Comparison of GP libraries\n",
    "\n",
    "We perform calculations using different GP implementations, which may serve as\n",
    "a reference.\n",
    "\n",
    "We test\n",
    "\n",
    "* [tinygp](https://github.com/dfm/tinygp)\n",
    "* [sklearn](https://scikit-learn.org)\n",
    "* [GPy](https://github.com/SheffieldML/GPy)\n",
    "* [gpytorch](https://gpytorch.ai)\n",
    "\n",
    "We perform the same calculations with all libraries and compare to the results\n",
    "obtained by a straight forward implementation of the textbook equations from\n",
    "{cite}`rasmussen_2006_GaussianProcessesMachine`.\n",
    "\n",
    "The values of the hyperparameters $\\ell$ and $\\sigma_n^2$ are usually the\n",
    "result of \"fitting the GP model to data\", which means optimizing the GP's log\n",
    "marginal likelihood as a function of both (e.g. what `sklearn`'s\n",
    "`GaussianProcessRegressor` does by default when `optimizer != None`).\n",
    "\n",
    "To ensure accurate comparisons, we\n",
    "\n",
    "* skip param optimization and instead fix $\\ell$ (kernel) and\n",
    "  $\\sigma_n^2$ (likelihood) because\n",
    "  * testing correct prediction code paths is orthogonal to how those are obtained\n",
    "  * codes use different optimizers and/or convergence thresholds and/or start\n",
    "    values, so optimized params might not be\n",
    "    * from the same (local) optimum of the log marginal likelihood\n",
    "    * equal enough numerically\n",
    "* skip code-internal data normalization\n",
    "* set regularization defaults to zero where needed to ensure that we only add\n",
    "  $\\sigma_n^2$ to the kernel matrix diag\n",
    "\n",
    "First, we generate reference textbook results. We compare each tested GP library\n",
    "against them using `numpy.testing` tools. Since we don't calculate with\n",
    "actual training data, we generate random data inputs and targets $(\\ve x_i\n",
    "\\in \\mathbb R^D, y_i \\in \\mathbb R)$.\n",
    "\n",
    "We calculate GP prior and posterior data in the `predict` and\n",
    "`predict_noiseless` setting:\n",
    "\n",
    "reference result name | meaning\n",
    "-|-\n",
    "text_pri          | prior, `predict_noiseless`\n",
    "text_pri_noise    | prior, `predict`\n",
    "text_pos          | posterior, `predict_noiseless`\n",
    "text_pos_noise    | posterior, `predict`\n",
    "\n",
    "Note that the `text_pri_noise` case is not useful in practice but we include\n",
    "it since some libraries expose the possibility to construct this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3187867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from common import (\n",
    "    textbook_prior,\n",
    "    textbook_posterior,\n",
    "    textbook_posterior_noise,\n",
    "    cov2std,\n",
    ")\n",
    "\n",
    "\n",
    "def rand(*size):\n",
    "    return rng.uniform(size=size)\n",
    "\n",
    "\n",
    "rng = np.random.default_rng(123)\n",
    "\n",
    "X_train = rand(100, 5)\n",
    "y_train = rand(100)\n",
    "X_pred = rand(50, 5)\n",
    "noise_level = 0.1\n",
    "length_scale = 1\n",
    "\n",
    "text_pri = textbook_prior(noise_level=0, length_scale=length_scale)(X_pred)\n",
    "text_pri_noise = textbook_prior(\n",
    "    noise_level=noise_level, length_scale=length_scale\n",
    ")(X_pred)\n",
    "\n",
    "text_pos = textbook_posterior(\n",
    "    X_train, y_train, noise_level=noise_level, length_scale=length_scale\n",
    ")(X_pred)\n",
    "text_pos_noise = textbook_posterior_noise(\n",
    "    X_train, y_train, noise_level=noise_level, length_scale=length_scale\n",
    ")(X_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cda87c",
   "metadata": {},
   "source": [
    "## tinygp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4de8b170",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "# =========================================================================\n",
    "# tinygp\n",
    "# =========================================================================\n",
    "import jax\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "from tinygp import GaussianProcess\n",
    "from tinygp.kernels import ExpSquared\n",
    "from tinygp.solvers import DirectSolver\n",
    "\n",
    "\n",
    "def compare_tinygp(gp, text_gp):\n",
    "    y_mean, y_std, y_cov = text_gp\n",
    "    np.testing.assert_allclose(y_mean, gp.loc)\n",
    "    np.testing.assert_allclose(y_std, np.sqrt(gp.variance))\n",
    "    np.testing.assert_allclose(y_cov, gp.covariance)\n",
    "\n",
    "\n",
    "# prior w/o noise\n",
    "gp = GaussianProcess(\n",
    "    kernel=ExpSquared(scale=length_scale),\n",
    "    X=X_pred,\n",
    "    diag=0,\n",
    ")\n",
    "compare_tinygp(gp, text_pri)\n",
    "\n",
    "# prior w/ noise\n",
    "gp = GaussianProcess(\n",
    "    kernel=ExpSquared(scale=length_scale),\n",
    "    X=X_pred,\n",
    "    diag=noise_level,\n",
    ")\n",
    "compare_tinygp(gp, text_pri_noise)\n",
    "\n",
    "# posterior, like GPy predict_noiseless() = R&W textbook eqns\n",
    "gp = GaussianProcess(\n",
    "    kernel=ExpSquared(scale=length_scale),\n",
    "    X=X_train,\n",
    "    solver=DirectSolver,\n",
    "    diag=noise_level,\n",
    ")\n",
    "cond = gp.condition(y_train, X_pred, diag=0)\n",
    "compare_tinygp(cond.gp, text_pos)\n",
    "\n",
    "# posterior, like GPy predict() -- add noise_level to y_cov's diag\n",
    "gp = GaussianProcess(\n",
    "    kernel=ExpSquared(scale=length_scale),\n",
    "    X=X_train,\n",
    "    solver=DirectSolver,\n",
    "    diag=noise_level,\n",
    ")\n",
    "cond = gp.condition(y_train, X_pred, diag=noise_level)\n",
    "compare_tinygp(cond.gp, text_pos_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a702982",
   "metadata": {},
   "source": [
    "## GPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bd7703f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning in stationary: failed to import cython module: falling back to numpy\n",
      "warning in coregionalize: failed to import cython module: falling back to numpy\n",
      "warning in choleskies: failed to import cython module: falling back to numpy\n"
     ]
    }
   ],
   "source": [
    "# =========================================================================\n",
    "# GPy\n",
    "# =========================================================================\n",
    "\n",
    "# Can't convince GPy to be more accurate than atol / rtol. There must be\n",
    "# hidden jitter defaults lurking around.\n",
    "\n",
    "\n",
    "def compare_gpy(pred_func, text_gp):\n",
    "    gp_mean, gp_cov = pred_func(X_pred, full_cov=True)\n",
    "    y_mean, y_std, y_cov = text_gp\n",
    "    np.testing.assert_allclose(y_mean, gp_mean[:, 0])\n",
    "    np.testing.assert_allclose(y_std, cov2std(gp_cov))\n",
    "    np.testing.assert_allclose(y_cov, gp_cov, rtol=1e-4)\n",
    "\n",
    "\n",
    "# posterior\n",
    "import GPy\n",
    "\n",
    "gpy_kernel = GPy.kern.RBF(\n",
    "    input_dim=X_train.shape[1],\n",
    "    lengthscale=length_scale,\n",
    "    variance=1,\n",
    "    inv_l=True,\n",
    ")\n",
    "gp = GPy.models.GPRegression(\n",
    "    X_train,\n",
    "    y_train[:, None],\n",
    "    gpy_kernel,\n",
    "    normalizer=False,\n",
    "    noise_var=noise_level,\n",
    ")\n",
    "\n",
    "# predict_noiseless()\n",
    "compare_gpy(gp.predict_noiseless, text_pos)\n",
    "\n",
    "# predict(): y_cov has noise_level added to the diag\n",
    "compare_gpy(gp.predict, text_pos_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e8a7ed",
   "metadata": {},
   "source": [
    "## GPyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "754543cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================\n",
    "# GPyTorch\n",
    "# =========================================================================\n",
    "\n",
    "import gpytorch\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.constraints import Positive\n",
    "import torch as T\n",
    "\n",
    "\n",
    "def compare_gpytorch(gp, text_gp):\n",
    "    y_mean, y_std, y_cov = text_gp\n",
    "    np.testing.assert_allclose(y_mean, gp.mean.detach().numpy())\n",
    "    np.testing.assert_allclose(y_std, np.sqrt(gp.variance.detach().numpy()))\n",
    "    np.testing.assert_allclose(\n",
    "        y_cov,\n",
    "        gp.covariance_matrix.detach().numpy(),\n",
    "    )\n",
    "\n",
    "\n",
    "def fixed(val):\n",
    "    return Positive(initial_value=val, transform=None, inv_transform=None)\n",
    "\n",
    "\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    \"\"\"API:\n",
    "    model.forward() -> prior\n",
    "    model()         -> posterior\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, y, likelihood):\n",
    "        super().__init__(X, y, likelihood)\n",
    "        kernel = gpytorch.kernels.RBFKernel(\n",
    "            lengthscale_constraint=fixed(length_scale),\n",
    "            eps=0,\n",
    "        )\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        self.covar_module = kernel\n",
    "\n",
    "    def forward(self, X):\n",
    "        return gpytorch.distributions.MultivariateNormal(\n",
    "            self.mean_module(X), self.covar_module(X)\n",
    "        )\n",
    "\n",
    "\n",
    "# Setting gpytorch.settings.linalg_dtypes() is not enough. We need to force\n",
    "# float64 torch-wide to get accurate results. In fact, this is the only\n",
    "# setting we need. We don't even need to set the cholesky_jitter to zero, so\n",
    "# probably gpytorch tries to solve w/o it first?\n",
    "T.set_default_dtype(T.float64)\n",
    "\n",
    "likelihood = GaussianLikelihood(noise_constraint=fixed(noise_level))\n",
    "model = ExactGPModel(T.from_numpy(X_train), T.from_numpy(y_train), likelihood)\n",
    "\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with (\n",
    "    T.no_grad(),\n",
    "    ##gpytorch.settings.cholesky_jitter(float=0, double=0, half=0),\n",
    "    ##gpytorch.settings.fast_computations(\n",
    "    ##    covar_root_decomposition=False, log_prob=False, solves=False\n",
    "    ##),\n",
    "    ##gpytorch.settings.fast_pred_var(False),\n",
    "    ##gpytorch.settings.linalg_dtypes(\n",
    "    ##    default=T.float64, symeig=T.float64, cholesky=T.float64\n",
    "    ##),\n",
    "):\n",
    "\n",
    "    # prior w/o noise: model.forward()\n",
    "    gp = model.forward(T.from_numpy(X_pred))\n",
    "    compare_gpytorch(gp, text_pri)\n",
    "\n",
    "    # prior w/ noise: likelihood(model.forward())\n",
    "    gp = likelihood(model.forward(T.from_numpy(X_pred)))\n",
    "    compare_gpytorch(gp, text_pri_noise)\n",
    "\n",
    "    # posterior, like GPy predict_noiseless(): model()\n",
    "    gp = model(T.from_numpy(X_pred))\n",
    "    compare_gpytorch(gp, text_pos)\n",
    "\n",
    "    # posterior, like GPy predict(): likelihood(model())\n",
    "    gp = likelihood(model(T.from_numpy(X_pred)))\n",
    "    compare_gpytorch(gp, text_pos_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbcd5ca",
   "metadata": {},
   "source": [
    "(s:gp_pred_comp_sklearn)=\n",
    "## sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f7fabee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================\n",
    "# sklearn\n",
    "# =========================================================================\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "\n",
    "# WhiteKernel API:\n",
    "#   k = WhiteKernel()\n",
    "#\n",
    "#   k(X) = np.eye(...) * noise_level\n",
    "#   k(X, X) = np.zeros(...)\n",
    "\n",
    "\n",
    "def compare_sklearn(gp, text_gp):\n",
    "    y_mean, y_std, y_cov = text_gp\n",
    "    gp_mean, gp_std = gp.predict(X_pred, return_std=True)\n",
    "    _, gp_cov = gp.predict(X_pred, return_cov=True)\n",
    "    np.testing.assert_allclose(y_mean, gp_mean)\n",
    "    np.testing.assert_allclose(y_std, gp_std)\n",
    "    np.testing.assert_allclose(y_cov, gp_cov)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# noise as regularization param, no noise in kernel\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# prior\n",
    "gp = GaussianProcessRegressor(\n",
    "    kernel=RBF(length_scale=length_scale),\n",
    "    alpha=noise_level,\n",
    "    optimizer=None,\n",
    "    normalize_y=False,\n",
    ")\n",
    "compare_sklearn(gp, text_pri)\n",
    "\n",
    "# posterior, like GPy predict_noiseless\n",
    "gp = gp.fit(X_train, y_train)\n",
    "compare_sklearn(gp, text_pos)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# noise as kernel param via WhiteKernel\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# prior, calling kernel(X) in predict() retains noise in kernel via\n",
    "# WhiteKernel\n",
    "gp = GaussianProcessRegressor(\n",
    "    kernel=RBF(length_scale=length_scale)\n",
    "    + WhiteKernel(noise_level=noise_level),\n",
    "    alpha=0,\n",
    "    optimizer=None,\n",
    "    normalize_y=False,\n",
    ")\n",
    "compare_sklearn(gp, text_pri_noise)\n",
    "\n",
    "# posterior, like GPy predict(), when using kernel(X, X) instead of kernel(X)\n",
    "# in sklearn's predict() then this is equal to GPy predict_noiseless()\n",
    "gp = gp.fit(X_train, y_train)\n",
    "compare_sklearn(gp, text_pos_noise)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.14.4"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "source_map": [
   16,
   18,
   72,
   107,
   111,
   166,
   170,
   210,
   214,
   300,
   305
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
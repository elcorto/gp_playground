% Encoding: UTF-8

@Online{deisenroth_2020_PracticalGuideGaussian,
  author   = {Marc Peter Deisenroth and Yicheng Luo and Mark van der Wilk},
  title    = {A Practical Guide to Gaussian Processes},
  year     = {2020},
  date     = {2020-12-03},
  url      = {https://infallible-thompson-49de36.netlify.app/},
  urldate  = {2020-12-03},
  keywords = {gp_play},
}

@Online{goertler_2019_VisualExplorationGaussiana,
  author       = {GÃ¶rtler, Jochen and Kehlbeck, Rebecca and Deussen, Oliver},
  title        = {A {{Visual Exploration}} of {{Gaussian Processes}}},
  date         = {2019-04-02},
  year         = {2019},
  url          = {https://distill.pub/2019/visual-exploration-gaussian-processes},
  urldate      = {2022-11-22},
  abstract     = {How to turn a collection of small building blocks into a versatile tool for solving regression problems.},
  doi          = {10.23915/distill.00017},
  issn         = {2476-0757},
  journaltitle = {Distill},
  keywords     = {gp_play},
  langid       = {english},
  number       = {4},
  pages        = {e17},
  volume       = {4},
}

@Online{kanagawa_2018_GaussianProcessesKernel,
  author        = {Kanagawa, Motonobu and Hennig, Philipp and Sejdinovic, Dino and Sriperumbudur, Bharath K.},
  title         = {Gaussian {{Processes}} and {{Kernel Methods}}: {{A Review}} on {{Connections}} and {{Equivalences}}},
  year          = {2018},
  url           = {http://arxiv.org/abs/1807.02582},
  abstract      = {This paper is an attempt to bridge the conceptual gaps between researchers working on the two widely used approaches based on positive definite kernels: Bayesian learning or inference using Gaussian processes on the one side, and frequentist kernel methods based on reproducing kernel Hilbert spaces on the other. It is widely known in machine learning that these two formalisms are closely related; for instance, the estimator of kernel ridge regression is identical to the posterior mean of Gaussian process regression. However, they have been studied and developed almost independently by two essentially separate communities, and this makes it difficult to seamlessly transfer results between them. Our aim is to overcome this potential difficulty. To this end, we review several old and new results and concepts from either side, and juxtapose algorithmic quantities from each framework to highlight close similarities. We also provide discussions on subtle philosophical and theoretical differences between the two approaches.},
  archiveprefix = {arXiv},
  eprint        = {1807.02582},
  eprinttype    = {arxiv},
  keywords      = {gp_play},
  primaryclass  = {cs, stat},
  shorttitle    = {Gaussian {{Processes}} and {{Kernel Methods}}},
}

@Book{murphy_2023_ProbabilisticMachineLearningAdvanced,
  author    = {Kevin P. Murphy},
  title     = {Probabilistic Machine Learning: Advanced Topics},
  year      = {2023},
  publisher = {MIT Press},
  url       = {https://probml.github.io/pml-book/book2.html},
  keywords  = {gp_play, mlbook},
}

@Book{rasmussen_2006_GaussianProcessesMachine,
  author    = {Carl Edward Rasmussen and Christopher K. I. Williams},
  title     = {Gaussian Processes for Machine Learning},
  year      = {2006},
  publisher = {MIT Press},
  isbn      = {ISBN-10 0-262-18253-X},
  url       = {http://www.gaussianprocess.org/gpml},
  keywords  = {gp_play},
}

@Book{bishop_2006_PatternRecognitionMachine,
  author    = {Bishop, Christopher},
  title     = {Pattern Recognition and Machine Learning},
  year      = {2006},
  publisher = {Springer},
  url       = {https://www.microsoft.com/en-us/research/publication/pattern-recognition-machine-learning/},
}

@Comment{jabref-meta: databaseType:biblatex;}

@Comment{jabref-meta: fileDirectory:".";}

@Comment{jabref-meta: saveOrderConfig:specified;bibtexkey;false;abstract;false;abstract;false;}

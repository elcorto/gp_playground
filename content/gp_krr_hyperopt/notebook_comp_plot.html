
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Explore GPs and KRR with code &#8212; gp_playground</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"ve": ["\\boldsymbol{\\mathit{#1}}", 1], "ma": ["\\mathbf{#1}", 1], "inv": ["{#1}^{-1}", 1], "trans": ["{#1}^{\\top}", 1], "cov": ["\\mathrm{cov}"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="prev" title="Compare GPs and kernel ridge regression (KRR)" href="intro.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">gp_playground</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../root.html">
                    <no title>
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../gp_pred_comp/intro.html">
   The role of noise in GPs
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../gp_pred_comp/notebook_common.html">
     Reference implementation of RW06 equations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../gp_pred_comp/notebook_comp.html">
     Comparison of GP libraries
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../gp_pred_comp/notebook_plot.html">
     Prior and (noisy) posterior predictions
    </a>
   </li>
  </ul>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="intro.html">
   Compare GPs and kernel ridge regression (KRR)
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Explore GPs and KRR with code
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/elcorto/gp_playground/main?urlpath=tree/book/content/gp_krr_hyperopt/notebook_comp_plot.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>
<a href="https://github.com/elcorto/gp_playground"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="bottom"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/content/gp_krr_hyperopt/notebook_comp_plot.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download notebook file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        <a href="../../_sources/content/gp_krr_hyperopt/notebook_comp_plot.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#functions-and-helpers">
   Functions and helpers
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data">
   Data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#correctness-checks-and-examples-on-how-to-play-with-optimizers">
   Correctness checks and examples on how to play with optimizers
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#plot-gp-and-krr-predictions-after-hyperopt">
   Plot GP and KRR predictions after hyperopt
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#plot-gp-and-krr-hyperparameter-objective-functions">
   Plot GP and KRR hyperparameter objective functions
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Explore GPs and KRR with code</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#functions-and-helpers">
   Functions and helpers
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data">
   Data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#correctness-checks-and-examples-on-how-to-play-with-optimizers">
   Correctness checks and examples on how to play with optimizers
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#plot-gp-and-krr-predictions-after-hyperopt">
   Plot GP and KRR predictions after hyperopt
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#plot-gp-and-krr-hyperparameter-objective-functions">
   Plot GP and KRR hyperparameter objective functions
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="explore-gps-and-krr-with-code">
<span id="s-gp-krr-code"></span><h1>Explore GPs and KRR with code<a class="headerlink" href="#explore-gps-and-krr-with-code" title="Permalink to this headline">#</a></h1>
<section id="functions-and-helpers">
<h2>Functions and helpers<a class="headerlink" href="#functions-and-helpers" title="Permalink to this headline">#</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import itertools
import multiprocessing as mp

##import multiprocess as mp

import numpy as np
from scipy.optimize import differential_evolution
import matplotlib.pyplot as plt
from matplotlib import cm, ticker, colors

from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, WhiteKernel
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import KFold
from sklearn.kernel_ridge import KernelRidge

from icecream import ic


def gt_func(x):
    &quot;&quot;&quot;Ground truth&quot;&quot;&quot;
    return np.sin(x) * np.exp(-0.1 * x) + 10


def gaussian_noise(x, rng, sigma=0.1):
    &quot;&quot;&quot;Constant Gaussian noise.&quot;&quot;&quot;
    return rng.normal(loc=0, scale=sigma, size=x.shape[0])


def transform_1d(scaler, x):
    assert x.ndim == 1
    return scaler.transform(x.reshape(-1, 1))[:, 0]


def de_callback(xk, convergence=None):
    &quot;&quot;&quot;Callback for differential_evolution that prints the best individual per
    iteration.&quot;&quot;&quot;
    ##ic(xk)


def de_callback_logscale(xk, convergence=None):
    &quot;&quot;&quot;Callback for differential_evolution that prints the best individual per
    iteration.

    Since GaussianProcessRegressor&#39;s hyper optimizer code path internally works
    with log(p) (= xk[0]) and log(r) (= xk[1]), we need to exp() them before
    printing.

    We also do that in HyperOpt if logscale=True.
    &quot;&quot;&quot;
    ##ic(np.exp(xk))


def gp_optimizer(obj_func, initial_theta, bounds):
    &quot;&quot;&quot;Custom optimizer for GaussianProcessRegressor using
    differential_evolution.

    Ignore initial_theta since we need only bounds for differential_evolution.
    &quot;&quot;&quot;
    # Avoid pickle error when using multiprocessing in
    # differential_evolution(..., workers=-1). We&#39;d use
    # https://github.com/uqfoundation/multiprocess instead of the stdlib&#39;s
    # multiprocessing to work around that but sadly differential_evolution()
    # uses the latter internally, so we&#39;re stuck with that.
    global _gp_obj_func_wrapper

    def _gp_obj_func_wrapper(params):
        ##print(f&quot;{obj_func(initial_theta)=}&quot;)
        # obj_func(theta, eval_gradient=True) hard-coded in
        # GaussianProcessRegressor, so it always returns the function value and
        # grad. However, we only need the function&#39;s value in
        # differential_evolution() below. obj_func = -log_marginal_likelihood.
        ##val, grad = obj_func(params)
        return obj_func(params)[0]

    opt_result = differential_evolution(
        ##lambda params: obj_func(params)[0],  # nope, no pickle for you
        _gp_obj_func_wrapper,
        bounds=bounds,
        callback=de_callback_logscale,
        **de_kwds_common,
    )
    return opt_result.x, opt_result.fun


##from sklearn.model_selection import cross_val_score
def simple_cv(model, X, y, cv):
    &quot;&quot;&quot;Same as

        -cross_val_score(model, X, y, cv=cv, scoring=&quot;neg_mean_squared_error&quot;)

    but much faster because we bypass the rich API of
    cross_val_score and can thus skip many many checks.

    Note that the double negative in -cross_val_score() and &quot;neg_&quot; is needed
    b/c of the sklearn API:

    &quot;All scorer objects follow the convention that higher return values are
    better than lower return values. Thus metrics which measure the distance
    between the model and the data, like metrics.mean_squared_error, are
    available as neg_mean_squared_error which return the negated value of the
    metric.&quot;

    See https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter
    &quot;&quot;&quot;
    errs = np.empty((cv.get_n_splits(X),), dtype=float)
    for ii, tup in enumerate(cv.split(X)):
        idxs_train, idxs_test = tup
        fm = model.fit(X[idxs_train, ...], y[idxs_train, ...])
        d = fm.predict(X[idxs_test, ...]) - y[idxs_test, ...]
        # MSE
        errs[ii] = np.dot(d, d) / len(d)
    return errs
    ##return -cross_val_score(model, X, y, cv=cv, scoring=&quot;neg_mean_squared_error&quot;)


class HyperOpt:
    &quot;&quot;&quot;Optimize hyper params of a sklearn model using
    differential_evolution.&quot;&quot;&quot;

    def __init__(self, bounds, get_model, logscale=False):
        self.bounds = bounds
        self.get_model = get_model
        self.logscale = logscale

    def obj_func(self, params, X, y):
        raise NotImplementedError

    def fit(self, X, y, return_params=False):
        global _ho_obj_func_wrapper

        if self.logscale:
            bounds_transform = lambda x: np.log(x)
            params_transform = lambda x: np.exp(x)
            callback = de_callback_logscale
        else:
            bounds_transform = lambda x: x
            params_transform = lambda x: x
            callback = de_callback

        def _ho_obj_func_wrapper(params):
            return self.obj_func(params_transform(params), X, y)

        opt_result = differential_evolution(
            _ho_obj_func_wrapper,
            bounds=bounds_transform(bounds),
            callback=callback,
            **de_kwds_common,
        )
        params = params_transform(opt_result.x)
        f = self.get_model(params).fit(X, y)
        if return_params:
            return f, params
        else:
            return f


class HyperOptKRR(HyperOpt):
    def __init__(self, *args, seed=None, **kwds):
        super().__init__(*args, **kwds)
        self.seed = seed

    def obj_func(self, params, X, y):
        cv = KFold(n_splits=5, random_state=self.seed, shuffle=True)
        return simple_cv(self.get_model(params), X, y, cv=cv).mean()


class HyperOptGP(HyperOpt):
    def obj_func(self, params, X, y):
        return -self.get_model(params).fit(X, y).log_marginal_likelihood()
</pre></div>
</div>
</div>
</div>
</section>
<section id="data">
<h2>Data<a class="headerlink" href="#data" title="Permalink to this headline">#</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>seed = 123
rng = np.random.default_rng(seed=seed)

# Equidistant x points: constant y_std (from GP) in-distribution
##x = np.linspace(0, 30, 60)
#
# Random x points for varying y_std. Also create a gap in the middle to
# show high y_std.
x1 = np.sort(rng.uniform(0, 12, 20), axis=0)
x2 = np.sort(rng.uniform(20, 32, 20), axis=0)
x = np.concatenate((x1, x2))
xspan = x.max() - x.min()
xi = np.linspace(x.min() - 0.3 * xspan, x.max() + 0.3 * xspan, len(x) * 10)
y = gt_func(x) + gaussian_noise(x, rng, sigma=0.075)
yi_gt = gt_func(xi)

# Data scaling.
in_scaler = StandardScaler().fit(x.reshape(-1, 1))
out_scaler = StandardScaler().fit(y.reshape(-1, 1))
x = transform_1d(in_scaler, x)
xi = transform_1d(in_scaler, xi)
y = transform_1d(out_scaler, y)
yi_gt = transform_1d(out_scaler, yi_gt)
X = x[:, None]
XI = xi[:, None]
</pre></div>
</div>
</div>
</div>
</section>
<section id="correctness-checks-and-examples-on-how-to-play-with-optimizers">
<h2>Correctness checks and examples on how to play with optimizers<a class="headerlink" href="#correctness-checks-and-examples-on-how-to-play-with-optimizers" title="Permalink to this headline">#</a></h2>
<p>This is actually the interesting part. Please inspect the code.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># -------------------------------------------------------------------------
# Sanity check KernelRidge API: alpha added to diag of kernel matrix.
# -------------------------------------------------------------------------
length_scale = 1
noise_level = 0.1
kp = RBF(length_scale=length_scale)
f_krr_1 = KernelRidge(alpha=noise_level, kernel=kp).fit(X, y)
f_krr_2 = KernelRidge(alpha=0, kernel=&quot;precomputed&quot;).fit(
    kp(X, X) + np.eye(X.shape[0]) * noise_level, y
)

np.testing.assert_allclose(f_krr_1.dual_coef_, f_krr_2.dual_coef_)
np.testing.assert_allclose(f_krr_1.predict(XI), f_krr_2.predict(kp(XI, X)))

# -------------------------------------------------------------------------
# Show that WhiteKernel(noise_level=) is equal to regularization param
# alpha=noise_level in both sklearn models. No hyperopt just yet
# (GaussianProcessRegressor(optimizer=None)). Use fixed hyper
# params=[length_scale,noise_level].
# -------------------------------------------------------------------------
f_gp_kp = GaussianProcessRegressor(
    kernel=kp,
    optimizer=None,
    normalize_y=False,
    alpha=noise_level,
).fit(X, y)

f_gp_kpr = GaussianProcessRegressor(
    kernel=RBF(length_scale=length_scale)
    + WhiteKernel(noise_level=noise_level),
    optimizer=None,
    normalize_y=False,
    alpha=0,
).fit(X, y)

np.testing.assert_allclose(f_gp_kp.alpha_, f_gp_kpr.alpha_)
np.testing.assert_allclose(f_gp_kp.predict(XI), f_gp_kpr.predict(XI))

np.testing.assert_allclose(f_gp_kp.alpha_, f_krr_1.dual_coef_)
np.testing.assert_allclose(f_gp_kp.predict(XI), f_krr_1.predict(XI))

# -------------------------------------------------------------------------
# Non-zero mean for fixed length_scale and noise_level.
# -------------------------------------------------------------------------
f_krr_nzm = KernelRidge(alpha=noise_level, kernel=kp).fit(X, y + 1000)
f_gp_nzm = GaussianProcessRegressor(
    kernel=RBF(length_scale=length_scale)
    + WhiteKernel(noise_level=noise_level),
    optimizer=None,
    normalize_y=False,
    alpha=0,
).fit(X, y + 1000)

np.testing.assert_allclose(f_gp_nzm.alpha_, f_krr_nzm.dual_coef_)
np.testing.assert_allclose(f_gp_nzm.predict(XI), f_krr_nzm.predict(XI))

# -------------------------------------------------------------------------
# hyperopt gp
#
# Show 4 ways to opt [length_scale, noise_level]. One using the default
# optimizer for reference. Then 3 ways to use the custom differential
# evolution (DE) based one using the HyperOpt helper class.
#
# DE results must be exactly equal if we use logscale=True, i.e. what
# GaussianProcessRegressor does internally, so these tests only check
# different code paths doing the same operations (of course also we fix all
# RNG seeds, so DE is reproducible).
#
# With logscale=False params are not equal but very close, also because we
# use polish=True which adds a final local optimizer run starting from the
# best DE result where we assume that we&#39;re close to the global opt and
# things are convex-ish.
# -------------------------------------------------------------------------
length_scale_bounds = (1e-5, 10)
noise_level_bounds = (1e-10, 10)
bounds = [length_scale_bounds, noise_level_bounds]

de_kwds_common = dict(
    polish=True,
    disp=False,
    atol=0,
    tol=0.001,
    popsize=20,
    maxiter=10000,
    workers=-1,
    updating=&quot;deferred&quot;,
    seed=seed,
)

# Internal optimizer API. Use default local optimizer (BFGS).
#
# For the wider bounds above the -LML shows big flat plateaus and so the
# local optimizer always goes off into the wild blue yonder with
# n_restarts_optimizer=0 (the default).
#
##ic(&quot;opt gp internal default RBF + WhiteKernel ...&quot;)
f_gp_0 = GaussianProcessRegressor(
    kernel=RBF(length_scale_bounds=length_scale_bounds)
    + WhiteKernel(
        noise_level_bounds=noise_level_bounds,
    ),
    n_restarts_optimizer=5,
    normalize_y=False,
    alpha=0,
).fit(X, y)
params_gp_0 = np.array(
    [
        f_gp_0.kernel_.k1.length_scale,
        f_gp_0.kernel_.k2.noise_level,
    ]
)
ic(params_gp_0)

# Internal optimizer API. Use differential_evolution.
#
##ic(&quot;opt gp internal RBF + WhiteKernel ...&quot;)
f_gp_1 = GaussianProcessRegressor(
    kernel=RBF(length_scale_bounds=length_scale_bounds)
    + WhiteKernel(
        noise_level_bounds=noise_level_bounds,
    ),
    n_restarts_optimizer=0,
    optimizer=gp_optimizer,
    normalize_y=False,
    alpha=0,
).fit(X, y)

params_gp_1 = np.array(
    [
        f_gp_1.kernel_.k1.length_scale,
        f_gp_1.kernel_.k2.noise_level,
    ]
)
ic(params_gp_1)

# External optimizer using HyperOpt helper class.
#
# RBF + WhiteKernel
#
##ic(&quot;opt gp external RBF + WhiteKernel ...&quot;)
get_model_gp_2 = lambda params: GaussianProcessRegressor(
    kernel=RBF(length_scale=params[0]) + WhiteKernel(noise_level=params[1]),
    optimizer=None,
    normalize_y=False,
    alpha=0,
)
f_gp_2, params_gp_2 = HyperOptGP(
    get_model=get_model_gp_2,
    bounds=bounds,
    logscale=True,
).fit(X, y, return_params=True)
ic(params_gp_2)

# External optimizer using HyperOpt helper class.
#
# RBF, alpha
#
##ic(&quot;opt gp external RBF + alpha ...&quot;)
get_model_gp_3 = lambda params: GaussianProcessRegressor(
    kernel=RBF(length_scale=params[0]),
    optimizer=None,
    normalize_y=False,
    alpha=params[1],
)
f_gp_3, params_gp_3 = HyperOptGP(
    get_model=get_model_gp_3,
    bounds=bounds,
    logscale=True,
).fit(X, y, return_params=True)
ic(params_gp_3)

np.testing.assert_allclose(params_gp_1, params_gp_2)
np.testing.assert_allclose(params_gp_1, params_gp_3)

np.testing.assert_allclose(f_gp_1.alpha_, f_gp_2.alpha_)
np.testing.assert_allclose(f_gp_1.alpha_, f_gp_3.alpha_)

np.testing.assert_allclose(f_gp_1.predict(XI), f_gp_2.predict(XI))
np.testing.assert_allclose(f_gp_1.predict(XI), f_gp_3.predict(XI))

# External optimizer using HyperOpt helper class.
#
# RBF, alpha
# logscale=False
#
##ic(&quot;opt gp external RBF + alpha nolog ...&quot;)
get_model_gp_3_nolog = lambda params: GaussianProcessRegressor(
    kernel=RBF(length_scale=params[0]),
    optimizer=None,
    normalize_y=False,
    alpha=params[1],
)
f_gp_3_nolog, params_gp_3_nolog = HyperOptGP(
    get_model=get_model_gp_3_nolog,
    bounds=bounds,
    logscale=False,
).fit(X, y, return_params=True)
ic(params_gp_3_nolog)

# -------------------------------------------------------------------------
# hyperopt krr
#
# KRR params will be different b/c CV != log_marginal_likelihood
# -------------------------------------------------------------------------
get_model_krr = lambda params: KernelRidge(
    alpha=params[1], kernel=RBF(length_scale=params[0])
)

##ic(&quot;opt krr RBF + alpha ...&quot;)
f_krr, params_krr = HyperOptKRR(
    bounds=bounds,
    get_model=get_model_krr,
    logscale=True,
    seed=seed,
).fit(X, y, return_params=True)
ic(params_krr)

##ic(&quot;opt krr RBF + alpha nolog ...&quot;)
f_krr_nolog, params_krr_nolog = HyperOptKRR(
    bounds=bounds,
    get_model=get_model_krr,
    logscale=False,
    seed=seed,
).fit(X, y, return_params=True)
ic(params_krr_nolog)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>ic| params_gp_0: array([0.11021769, 0.05521955])
ic| params_gp_1: array([0.11021771, 0.05521953])
ic| params_gp_2: array([0.11021771, 0.05521953])
ic| params_gp_3: array([0.11021771, 0.05521953])
ic| params_gp_3_nolog: array([0.11021768, 0.05521953])
ic| params_krr: array([0.15470176, 0.01443063])
ic| params_krr_nolog: array([0.15470279, 0.01442918])
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.15470279, 0.01442918])
</pre></div>
</div>
</div>
</div>
</section>
<section id="plot-gp-and-krr-predictions-after-hyperopt">
<h2>Plot GP and KRR predictions after hyperopt<a class="headerlink" href="#plot-gp-and-krr-predictions-after-hyperopt" title="Permalink to this headline">#</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># -------------------------------------------------------------------------
# Plot functions, data and GP&#39;s std
# -------------------------------------------------------------------------
plt.rcParams[&quot;figure.autolayout&quot;] = True
plt.rcParams[&quot;font.size&quot;] = 18
savefig = False

yi_krr = f_krr.predict(XI)
yi_gp, yi_gp_std = f_gp_1.predict(XI, return_std=True)
##yi_gp, yi_gp_std = f_gp_3_nolog.predict(XI, return_std=True)

fig1, axs = plt.subplots(
    nrows=2,
    sharex=True,
    gridspec_kw=dict(height_ratios=[1, 0.2]),
    figsize=(15, 8),
)
axs[0].plot(x, y, &quot;o&quot;, color=&quot;tab:gray&quot;)
axs[0].plot(xi, yi_krr, label=&quot;KRR&quot;, color=&quot;tab:red&quot;, lw=2)
axs[0].plot(xi, yi_gp, label=&quot;GP&quot;, color=&quot;tab:green&quot;, lw=2)
axs[0].fill_between(
    xi,
    yi_gp - 2 * yi_gp_std,
    yi_gp + 2 * yi_gp_std,
    alpha=0.1,
    color=&quot;tab:green&quot;,
    label=r&quot;GP $\pm 2\,\sigma$&quot;,
)

yspan = y.max() - y.min()
axs[0].plot(xi, yi_gt, label=&quot;Ground truth g(x)&quot;, color=&quot;tab:gray&quot;, alpha=0.5)
axs[0].set_ylim(y.min() - 0.5 * yspan, y.max() + 0.5 * yspan)

axs[1].plot(xi, yi_gp_std, label=r&quot;GP $\sigma$&quot;)

for ax in axs:
    ax.legend()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/notebook_comp_plot_9_0.png" src="../../_images/notebook_comp_plot_9_0.png" />
</div>
</div>
</section>
<section id="plot-gp-and-krr-hyperparameter-objective-functions">
<h2>Plot GP and KRR hyperparameter objective functions<a class="headerlink" href="#plot-gp-and-krr-hyperparameter-objective-functions" title="Permalink to this headline">#</a></h2>
<p>So here we have it. -LML <span class="math notranslate nohighlight">\(\neq\)</span> CV, and therefore the optimal hyperparameters
<span class="math notranslate nohighlight">\((\ell,\eta)\)</span> are different.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Could re-use HyperOpt instances from above, but re-create here for
# clarity.
##ic(&quot;Plot hyperopt objective functions&quot;)
nsample = 35
nlevels = 35
z_log = False
length_scale = np.logspace(*np.log10(length_scale_bounds), nsample)
noise_level = np.logspace(*np.log10(noise_level_bounds), nsample)
grid = np.array(list(itertools.product(length_scale, noise_level)))
fig2, axs2d = plt.subplots(nrows=1, ncols=2, figsize=(18, 8))
fig3, axs3d = plt.subplots(
    nrows=1, ncols=2, figsize=(18, 8), subplot_kw={&quot;projection&quot;: &quot;3d&quot;}
)

# zmax for linear z scale (see below)
cases = dict(gp=dict(zmax=1e-9), krr=dict(zmax=1e-5))

for icol, name in enumerate(cases):
    ax2d = axs2d[icol]
    ax3d = axs3d[icol]
    ax2d.set_title(name.upper())
    ax3d.set_title(name.upper())
    if name == &quot;krr&quot;:
        ho = HyperOptKRR(
            bounds=bounds,
            get_model=get_model_krr,
            seed=seed,
        )

        def func(params):
            return ho.obj_func(params, X, y)

        params_opt = params_krr
    elif name == &quot;gp&quot;:

        # Use a fitted GP object from above that has RBF+WhiteKernel such
        # that we can call log_marginal_likelihood() with a length 2 param
        # array.
        #
        def func(params):
            return -f_gp_1.log_marginal_likelihood(np.log(params))

        # Use HyperOpt API defined above.
        #
        ##ho = HyperOptGP(
        ##    get_model=get_model_gp_2,
        ##    bounds=bounds,
        ##)

        ##def func(params):
        ##    return ho.obj_func(params, X, y)

        params_opt = params_gp_1

    with mp.Pool(mp.cpu_count()) as pool:
        zz = np.array(pool.map(func, grid))

    # z log scale looks nice but is hard to interpret, also it heavily
    # depends on eps of course. When not using a z log scale we need to cut
    # off at z &gt;= zmax to visualize low z value regions where the global
    # mins live. Note that zmax depends on bounds and thus on the range of
    # z values. Compared to krr, the GP&#39;s LML is essentially flat around
    # the min. Still DE and the local optimizer find the same min.
    if z_log:
        eps = 0.01
        zz -= zz.min() - eps
        zz /= zz.max()
    else:
        zmax = cases[name][&quot;zmax&quot;]
        zz -= zz.min()
        zz /= zz.max()
        ##zz = np.ma.masked_where(zz &gt; zmax, zz)
        zz[zz &gt;= zmax] = zmax

    _X, _Y = np.meshgrid(length_scale, noise_level, indexing=&quot;ij&quot;)
    Z = zz.reshape((_X.shape[0], _X.shape[1]))

    if z_log:
        levels = np.logspace(np.log10(zz.min()), np.log10(zz.max()), nlevels)
        pl2d = ax2d.contourf(_X, _Y, Z, levels=levels, norm=colors.LogNorm())
        pl3d = ax3d.plot_surface(
            np.log10(_X), np.log10(_Y), np.log10(Z), cmap=cm.viridis
        )
    else:
        pl2d = ax2d.contourf(_X, _Y, Z, levels=nlevels)
        pl3d = ax3d.plot_surface(
            np.log10(_X),
            np.log10(_Y),
            Z,
            cmap=cm.viridis,
        )
    ax3d.view_init(elev=40, azim=-140, roll=0)

    fig2.colorbar(pl2d, ax=ax2d)
    fig3.colorbar(pl3d, ax=ax3d)
    ax2d.plot(*params_opt, &quot;o&quot;, ms=10, color=&quot;white&quot;)
    if name == &quot;gp&quot;:
        ax2d.plot(*params_gp_0, &quot;*&quot;, ms=10, color=&quot;black&quot;)
    ax2d.set_xlabel(r&quot;$\ell$&quot;)
    ax2d.set_ylabel(r&quot;$\sigma_n^2$&quot;)
    ax3d.set_xlabel(r&quot;$\log_{10}(\ell)$&quot;)
    ax3d.set_ylabel(r&quot;$\log_{10}(\sigma_n^2)$&quot;)
    ax2d.set_xscale(&quot;log&quot;)
    ax2d.set_yscale(&quot;log&quot;)

if savefig:
    fig1.savefig(&quot;gp_krr_pred.pdf&quot;)
    fig2.savefig(&quot;gp_krr_hyperopt_objective.pdf&quot;)
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/notebook_comp_plot_11_0.png" src="../../_images/notebook_comp_plot_11_0.png" />
<img alt="../../_images/notebook_comp_plot_11_1.png" src="../../_images/notebook_comp_plot_11_1.png" />
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content/gp_krr_hyperopt"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="intro.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Compare GPs and kernel ridge regression (KRR)</p>
        </div>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Steve Schmerler<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>